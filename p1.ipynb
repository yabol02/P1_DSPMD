{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e19ba33",
   "metadata": {},
   "source": [
    "# Diseño de Sistemas de Adquisición y Procesamiento Masivo de Datos\n",
    "\n",
    "## Práctica 1\n",
    "\n",
    "### Yago Boleas y Zakaria Lasry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc28ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import polars as pl\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "import plotly.express as px\n",
    "import plotly.graph_objs as go\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import clean_lidar_points, make_voxel_grid, mark_static_points, clusters_with_dbscan, compute_centroids\n",
    "from utils import update_tracking_state, predict_positions, compute_velocities, match_clusters, plot_scene, create_bounding_box_lines\n",
    "from utils import max_, min_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb6b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict(\n",
    "    carretera=Path(\"./data/0_carretera.csv\"),\n",
    "    portico=Path(\"./data/1_portico.csv\"),\n",
    "    coche=Path(\"./data/2_coche.csv\"),\n",
    "    coches=Path(\"./data/3_coche_coche.csv\"),\n",
    "    coches_moto=Path(\"./data/4_coche_coche_moto.csv\"),\n",
    "    video=Path(\"./data/5_video_nube_de_puntos\"),\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6525bdef",
   "metadata": {},
   "source": [
    "## ANÁLISIS DE LOS DATOS\n",
    "\n",
    "Lo primero es echar un vistazo a las distintas variables que hay en los conjuntos de datos. Usamos el primer conjunto de datos `carretera` para esto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c0067d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(data[\"carretera\"])\n",
    "df = df.unique([\"x\", \"y\", \"z\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd2682",
   "metadata": {},
   "source": [
    "### Descripción de variables\n",
    "\n",
    "- **x**: coordenada cartesiana en el eje horizontal (eje X).\n",
    "- **y**: coordenada cartesiana en el eje horizontal (eje Y).\n",
    "- **z**: coordenada cartesiana en el eje vertical (eje Z).\n",
    "- **intensity**: intensidad de la señal reflejada que regresa al sensor. Indica cuánta luz láser es reflejada por el objeto detectado.\n",
    "- **t**: marca de tiempo (*timestamp*) en la que se tomó la medición.\n",
    "- **reflectivity**: reflectividad del objeto detectado. Mide la capacidad del objeto para reflejar la luz láser.\n",
    "- **ring**: índice del anillo o línea de escaneo al que pertenece el punto.\n",
    "- **ambient**: nivel de luz ambiental presente en el entorno. \n",
    "- **range**: distancia desde el sensor Lidar hasta el objeto detectado, medida en metros.\n",
    "\n",
    "### Primer vistazo a los datos\n",
    "\n",
    "En esta figura podemos ver la carretera sin ningún tipo de obtáculo por en medio. Se ha decidido también representar la variable `range`, que representa la distancia a la que se encuentra cada uno de los puntos al sensor Lidar. Se muestra también el punto (0, 0, 0) para poder apreciar con mayor claridad esta variable.\n",
    "\n",
    "Para representar los datos también se ha realizado un pequeño procesado de los datos, compuesto por dos etapas:\n",
    "\n",
    "1. **Eliminación de duplicados**: se eliminaron los puntos que tenían coordenadas `x`, `y` y `z` idénticas.\n",
    "2. **Filtrado de puntos atípicos**: se aplicó un método estadístico basado en el *Rango Intercuartílico (IQR)* para detectar y eliminar los puntos que estaban demasiado lejos del grupo principal. Específicamente, se eliminaron todos aquellos puntos que se encontraban a una distancia mayor de 2 veces el IQR por debajo del cuartil 25 o por encima del cuartil 75 para cada eje `(x, y, z)`.\n",
    "\n",
    "Además de la limpieza, se ha representado la variable `range`, que indica la distancia de cada punto al sensor LiDAR, utilizando una **escala de colores** ('*hot*'). Para facilitar la comprensión espacial, se ha añadido un punto de referencia en el origen `(0, 0, 0)`. Es importante notar que, para la visualización, se han ajustado los ejes para una mejor perspectiva: el *eje Z* del gráfico representa la coordenada `y` del sensor (invertida) y el *eje Y* del gráfico la coordenada `z`.  Finalmente, los rangos de los ejes se han limitado a `[-30, 30]` para centrar la vista en el área de interés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50374924",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"carretera\"\n",
    "color_var = \"range\"\n",
    "df = pl.read_csv(data[dataset])\n",
    "df = clean_lidar_points(df, verbose=False)\n",
    "fig = plot_scene(df, color_var, dataset, pov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987c5955",
   "metadata": {},
   "source": [
    "### Algoritmo de eliminación de puntos estáticos\n",
    "\n",
    "En esta nueva escena se emplean de nuevo los datos de la carretera vacía. El objetivo de esta parte es obtener una representación de las zonas estáticas dentro de la visión del Lidar. Para ello, se implementa un método de **voxelización** para crear un modelo estático del entorno. Cuando llegue una nueva escena, se comprobará que los datos pertenezcan o no a una de las distintas regiones. En caso de pertenecer a esta zona no se tendrán en cuenta a la hora de hacer la clusterización. El proceso para obtener esa escena estática es el siguiente:\n",
    "\n",
    "1.  **Voxelización del espacio**: se define una región de interés en el espacio tridimensional (un cubo de $60 \\times 60 \\times 60$ metros, de -30 a 30 en cada eje) y se divide en una rejilla de $120 \\times 120 \\times 120$ voxels (un voxel es el equivalente tridimensional de un píxel).\n",
    "2.  **Asignación de puntos a voxels**: cada punto de la nube de puntos se asigna a un voxel específico. Esto se logra calculando las coordenadas del voxel (`vx`, `vy`, `vz`) para cada punto basándose en sus coordenadas `x`, `y` y `z` y el tamaño del voxel.\n",
    "3.  **Conteo de puntos por voxel**: se cuenta el número de puntos de la nube que caen dentro de cada voxel. Este conteo se almacena en el array tridimensional `voxel`. Un voxel con un conteo mayor a cero indica que esa región del espacio está \"ocupada\" por un objeto estático.\n",
    "\n",
    "Para dar una idea del resultado de la voxelización se muestra una figura tridimensional con todos los voxels hallados, donde:\n",
    "\n",
    "-   El **tamaño** de los marcadores (cada uno de los puntos) en el gráfico está escalado en función del tamaño del voxel, lo que da una idea de la granularidad de la rejilla.\n",
    "-   El **color** de cada marcador representa el **conteo de puntos** en ese voxel. Esto permite identificar áreas con mayor densidad de puntos, como las superficies de la carretera o la estructura del pórtico.\n",
    "-   Para una visualización más intuitiva, los ejes del gráfico se han intercambiado (`x` $\\Rightarrow$ `-vx`, `y` $\\Rightarrow$ `vz`, `z` $\\Rightarrow$ `-vy`) para que el pórtico se muestre en una orientación más convencional.\n",
    "\n",
    "Este modelo de voxels sirve como una \"huella\" de la escena estática. En el futuro, cuando llegue una nueva nube de puntos, cualquier punto que no caiga en un voxel previamente ocupado se considera parte de un **objeto móvil**, lo que facilita su detección y aislamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1b33f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_pts = pl.read_csv(data[\"carretera\"])\n",
    "static_pts = static_pts.unique(subset=[\"x\", \"y\", \"z\"])\n",
    "regions = 120\n",
    "voxels = make_voxel_grid(static_pts, n_regions=regions)\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Scatter3d(\n",
    "            x=-voxels[\"vx\"],\n",
    "            y=voxels[\"vz\"],\n",
    "            z=-voxels[\"vy\"],\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=5 * ((max_ - min_) / regions),\n",
    "                color=voxels[\"count\"],\n",
    "                colorbar=dict(\n",
    "                    title=\"count\",\n",
    "                    tickfont=dict(size=12),\n",
    "                    title_font=dict(size=14),\n",
    "                ),\n",
    "                opacity=0.8,\n",
    "            ),\n",
    "            hoverinfo=\"text\",\n",
    "            hovertext=voxels[\"count\"],\n",
    "        )\n",
    "    ],\n",
    "    layout=go.Layout(\n",
    "        width=800,\n",
    "        height=800,\n",
    "        scene=dict(\n",
    "            xaxis=dict(range=[-regions, 0]),\n",
    "            yaxis=dict(range=[0, regions]),\n",
    "            zaxis=dict(range=[-regions, 0]),\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c1d442",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Pórtico (con coche)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c518ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_var = \"cluster\"\n",
    "dataset = \"portico\"\n",
    "\n",
    "df2 = pl.read_csv(data[dataset])\n",
    "df2 = clean_lidar_points(df2, verbose=False)\n",
    "df2 = mark_static_points(df2, voxels, n_regions=regions)\n",
    "df2 = clusters_with_dbscan(\n",
    "    df2,\n",
    "    [\"x\", \"y\", \"z\"],\n",
    ")\n",
    "\n",
    "plot_scene(df2, color_var, dataset, pov=False, opacity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f9e21c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Un coche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4b85fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_var = \"cluster\"\n",
    "dataset = \"coche\"\n",
    "\n",
    "df3 = pl.read_csv(data[dataset])\n",
    "df3 = clean_lidar_points(df3, verbose=False)\n",
    "df3 = mark_static_points(df3, voxels, n_regions=regions)\n",
    "df3 = clusters_with_dbscan(\n",
    "    df3,\n",
    "    [\"x\", \"y\", \"z\"],\n",
    ")\n",
    "plot_scene(df3, color_var, dataset, pov=True, opacity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f0d8c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dos coches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f7bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_var = \"cluster\"\n",
    "dataset = \"coches\"\n",
    "\n",
    "df4 = pl.read_csv(data[dataset])\n",
    "df4 = clean_lidar_points(df4, verbose=False)\n",
    "df4 = mark_static_points(df4, voxels, n_regions=regions)\n",
    "df4 = clusters_with_dbscan(\n",
    "    df4,\n",
    "    [\"x\", \"y\", \"z\"],\n",
    ")\n",
    "plot_scene(df4, color_var, dataset, pov=True, opacity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4be259",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Dos coches y moto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee712a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_var = \"cluster\"\n",
    "dataset = \"coches_moto\"\n",
    "\n",
    "df5 = pl.read_csv(data[dataset])\n",
    "df5 = clean_lidar_points(df5, verbose=False)\n",
    "df5 = mark_static_points(df5, voxels, n_regions=regions)\n",
    "df5 = clusters_with_dbscan(\n",
    "    df5,\n",
    "    [\"x\", \"y\", \"z\"],\n",
    ")\n",
    "plot_scene(df5, color_var, dataset, pov=True, opacity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ca792",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Uso del algoritmo KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a80efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kmeans = df5.filter(pl.col(\"cluster\") >= -1)\n",
    "\n",
    "K = df5.filter(pl.col(\"cluster\") >= 0).select(pl.col(\"cluster\").unique().len()).item()\n",
    "print(f\"Número de clústeres detectados por DBSCAN: {K}\")\n",
    "\n",
    "X = df5.filter(pl.col(\"cluster\") >= -1).select([\"x\", \"y\", \"z\"]).to_numpy()\n",
    "kmeans = KMeans(n_clusters=K, random_state=0).fit(X)\n",
    "\n",
    "df_kmeans = df_kmeans.with_columns(pl.Series(\"kmeans_cluster\", kmeans.labels_))\n",
    "plot_scene(df_kmeans, \"kmeans_cluster\", dataset, pov=True, opacity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658cb4fc",
   "metadata": {},
   "source": [
    "\n",
    "Una vez vistos estos resultados, vemos que en este caso aplicar **KMeans** carece de sentido práctico por varias razones fundamentales: En primer lugar, el algoritmo exige conocer el número de clústeres $K$ antes de ejecutarse, lo cual no es viable para el caso a evaluar. El otro gran problema se puede apreciar al mostrar los datos de forma gráfica. En caso de no limpiar con una precisión impecable los datos, el algoritmo **KMeans** si o si clasificará puntos de ruido en algún clúster, los cuales con DBSCAN son tratados como lo que son. Otro de los fundamentos para evitar este algoritmo es que no hace una separación ideal de los clústeres. En la figura superior se puede ver cómo en uno de los 3 clústeres que salieron del algoritmo hay dos vehículos, cuando el objetivo de aplicar técnicas de *clustering* en este caso es separar ambos elementos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db68bf1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04777240",
   "metadata": {},
   "source": [
    "## 6. Tracking de vehículos\n",
    "\n",
    "El **tracking** consiste en identificar y mantener la trayectoria de cada objeto móvil a lo largo del tiempo, asignándole un identificador único y monitoreando su ubicación y estado continuamente. De esta forma es posible reconstruir el movimiento de los vehículos a partir de las nubes de puntos captadas por el sensor. El algoritmo empleado para realizar el tracking sobre los vehículos consiste en lo siguiente:\n",
    "\n",
    "1.  **Clusterización de cada fotograma**: los puntos de cada escena se agrupan mediante **DBSCAN**, usando las coordenadas `(x, y, z)` para identificar los distintos vehículos y descartar el ruido (etiquetado como `-1`).\n",
    "\n",
    "2.  **Cálculo de centroides**: para cada cluster válido se calcula el **centro geométrico** en el plano `(x, y)`, tomando el punto medio entre los valores mínimos y máximos de cada eje. Este centro representa la posición estimada del vehículo en el fotograma. Se calcula este centro geométrico y no el centroide del cluster identificado debido a que este depende de la concentración de los puntos, por lo que podría causar inconsistencias en las zonas más cercanas al sensor por estar los puntos mucho más condensados.\n",
    "\n",
    "3.  **Emparejamiento entre fotogramas**: los centroides del fotograma actual se comparan con las **posiciones predichas** del fotograma anterior. Se calcula la distancia entre todos los centroides y se emparejan los más cercanos si la distancia es inferior a un umbral, identificando así los mismos vehículos a lo largo del tiempo.\n",
    "\n",
    "4.  **Asignación de identificadores de seguimiento**: cada cluster recibe un `track_id`. Si un cluster actual se empareja con uno previo, mantiene su identificador; si no, se le asigna uno nuevo. Esto asegura la continuidad del seguimiento en toda la secuencia.\n",
    "\n",
    "5.  **Cálculo de velocidades**: una vez emparejados los clusters, se estima la velocidad de cada vehículo a partir de la diferencia entre sus posiciones consecutivas, normalizada por el intervalo temporal entre fotogramas.\n",
    "\n",
    "6.  **Predicción de posiciones futuras**: utilizando las velocidades calculadas, se predicen las posiciones de cada vehículo en el siguiente fotograma. Estas predicciones facilitan el emparejamiento incluso ante ligeras oclusiones o movimientos rápidos.\n",
    "\n",
    "7.  **Actualización del estado de tracking**: se registra en un DataFrame todas las variables relevantes (`cluster`, `track_id`, posición, velocidad y predicción) para cada fotograma, permitiendo reconstruir la trayectoria de todos los vehículos.\n",
    "\n",
    "Al final del proceso, cada nube de puntos queda enriquecida con la información de cluster y seguimiento, permitiendo analizar el movimiento individual de los vehículos y su evolución temporal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "lazy_dfs = [\n",
    "    pl.scan_csv(os.path.join(data[\"video\"], file)).with_columns(\n",
    "        pl.lit(i).alias(\"frame\")\n",
    "    )\n",
    "    for i, file in enumerate(sorted(os.listdir(data[\"video\"])))\n",
    "]\n",
    "df_raw = pl.concat(lazy_dfs).collect()\n",
    "df_raw = df_raw.filter(pl.col(\"x\") < 0)\n",
    "frames = df_raw[\"frame\"].unique().sort().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fc4155",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=2, min_samples=20)\n",
    "\n",
    "processed_dfs = []\n",
    "prev_centroids, prev_preds = {}, {}\n",
    "next_track_id = 0\n",
    "prev_id_to_track_id = {}\n",
    "tracking_state = pl.DataFrame(\n",
    "    schema={\n",
    "        \"frame\": pl.Int64,\n",
    "        \"cluster\": pl.Int64,\n",
    "        \"cx\": pl.Float64,\n",
    "        \"cy\": pl.Float64,\n",
    "        \"vx\": pl.Float64,\n",
    "        \"vy\": pl.Float64,\n",
    "        \"px\": pl.Float64,\n",
    "        \"py\": pl.Float64,\n",
    "    }\n",
    ")\n",
    "\n",
    "for frame_id in tqdm(frames, desc=\"Tracking vehicles\"):\n",
    "    frame_df = df_raw.filter(pl.col(\"frame\") == frame_id)\n",
    "    points = frame_df.select([\"x\", \"y\", \"z\"]).to_numpy()\n",
    "\n",
    "    labels = dbscan.fit_predict(points)\n",
    "    centroids = compute_centroids(points, labels)\n",
    "\n",
    "    matches = match_clusters(prev_preds, centroids, threshold=2.5)\n",
    "    current_id_to_track_id = {}\n",
    "\n",
    "    for new_id in centroids.keys():\n",
    "        prev_id = matches.get(new_id)\n",
    "        if prev_id is not None and prev_id in prev_id_to_track_id:\n",
    "            track_id = prev_id_to_track_id[prev_id]\n",
    "        else:\n",
    "            track_id = next_track_id\n",
    "            next_track_id += 1\n",
    "        current_id_to_track_id[new_id] = track_id\n",
    "\n",
    "    track_ids = (\n",
    "        pl.Series(\"track_id\", labels).replace(current_id_to_track_id).fill_null(-1)\n",
    "    )\n",
    "    processed_dfs.append(frame_df.with_columns(pl.Series(\"cluster\", labels), track_ids))\n",
    "\n",
    "    velocities = compute_velocities(prev_centroids, centroids, matches)\n",
    "    preds = predict_positions(centroids, velocities)\n",
    "\n",
    "    tracking_state = update_tracking_state(\n",
    "        tracking_state,\n",
    "        frame_id=frame_id,\n",
    "        centroids=centroids,\n",
    "        velocities=velocities,\n",
    "        preds=preds,\n",
    "    )\n",
    "\n",
    "    prev_centroids = centroids\n",
    "    prev_preds = preds\n",
    "    prev_id_to_track_id = current_id_to_track_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325b949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustered = pl.concat(processed_dfs)\n",
    "\n",
    "x_min, x_max = df_clustered[\"x\"].min(), df_clustered[\"x\"].max()\n",
    "y_min, y_max = df_clustered[\"y\"].min(), df_clustered[\"y\"].max()\n",
    "z_min, z_max = df_clustered[\"z\"].min(), df_clustered[\"z\"].max()\n",
    "\n",
    "padding_factor = 1\n",
    "x_range = x_max - x_min\n",
    "y_range = y_max - y_min\n",
    "z_range = z_max - z_min\n",
    "\n",
    "x_min -= x_range * padding_factor\n",
    "x_max += x_range * padding_factor\n",
    "y_min -= y_range * padding_factor\n",
    "y_max += y_range * padding_factor\n",
    "z_min -= z_range * padding_factor\n",
    "z_max += z_range * padding_factor\n",
    "\n",
    "colors = px.colors.qualitative.Plotly\n",
    "animation_frames = []\n",
    "for f in tqdm(frames, desc=\"Adding bounding boxes to the frames\"):\n",
    "    frame_traces = []\n",
    "    current_frame_data = df_clustered.filter(pl.col(\"frame\") == f)\n",
    "\n",
    "    noise_points = current_frame_data.filter(pl.col(\"track_id\") == -1)\n",
    "    if not noise_points.is_empty():\n",
    "        frame_traces.append(\n",
    "            go.Scatter3d(\n",
    "                x=noise_points[\"x\"],\n",
    "                y=noise_points[\"y\"],\n",
    "                z=noise_points[\"z\"],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=1.5, color=\"grey\", opacity=0.5),\n",
    "                name=\"Noise\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    tracked_ids = (\n",
    "        current_frame_data.filter(pl.col(\"track_id\") >= 0)[\"track_id\"]\n",
    "        .unique()\n",
    "        .sort()\n",
    "        .to_list()\n",
    "    )\n",
    "\n",
    "    for track_id in tracked_ids:\n",
    "        cluster_points = current_frame_data.filter(pl.col(\"track_id\") == track_id)\n",
    "        color = colors[track_id % len(colors)]\n",
    "\n",
    "        frame_traces.append(\n",
    "            go.Scatter3d(\n",
    "                x=cluster_points[\"x\"],\n",
    "                y=cluster_points[\"y\"],\n",
    "                z=cluster_points[\"z\"],\n",
    "                mode=\"markers\",\n",
    "                marker=dict(size=2, color=color),\n",
    "                name=f\"Track ID {track_id}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "        min_vals = cluster_points.select([\"x\", \"y\", \"z\"]).min().row(0)\n",
    "        max_vals = cluster_points.select([\"x\", \"y\", \"z\"]).max().row(0)\n",
    "        x_lines, y_lines, z_lines = create_bounding_box_lines(\n",
    "            min_vals[0], max_vals[0], min_vals[1], max_vals[1], min_vals[2], max_vals[2]\n",
    "        )\n",
    "        frame_traces.append(\n",
    "            go.Scatter3d(\n",
    "                x=x_lines,\n",
    "                y=y_lines,\n",
    "                z=z_lines,\n",
    "                mode=\"lines\",\n",
    "                line=dict(color=color, width=2.5),\n",
    "                name=f\"Box {track_id}\",\n",
    "            )\n",
    "        )\n",
    "\n",
    "    animation_frames.append(go.Frame(data=frame_traces, name=str(f)))\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=animation_frames[0].data if animation_frames else [],\n",
    "    layout=go.Layout(\n",
    "        width=900,\n",
    "        height=700,\n",
    "        scene=dict(\n",
    "            # Use the specific ranges for each axis\n",
    "            xaxis=dict(title=\"X\", range=[x_min, x_max]),\n",
    "            yaxis=dict(title=\"Y\", range=[y_min, y_max]),\n",
    "            zaxis=dict(title=\"Z\", range=[z_min, z_max]),\n",
    "            # Change aspectmode to allow axes to fit their data\n",
    "            aspectmode=\"data\",\n",
    "        ),\n",
    "        legend=dict(itemsizing=\"constant\", font=dict(size=10)),\n",
    "    ),\n",
    "    frames=animation_frames,\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text=\"Lidar Point Cloud with Persistent Cluster Tracking\",\n",
    "        x=0.5,\n",
    "        y=0.95,\n",
    "        xanchor=\"center\",\n",
    "        yanchor=\"top\",\n",
    "        font=dict(size=24),\n",
    "    ),\n",
    "    font=dict(family=\"Arial, monospace\", size=12, color=\"Black\"),\n",
    "    sliders=[\n",
    "        dict(\n",
    "            steps=[\n",
    "                dict(\n",
    "                    args=[\n",
    "                        [str(f)],\n",
    "                        dict(frame={\"duration\": 200, \"redraw\": True}, mode=\"immediate\"),\n",
    "                    ],\n",
    "                    label=str(f),\n",
    "                    method=\"animate\",\n",
    "                )\n",
    "                for f in frames\n",
    "            ],\n",
    "            transition={\"duration\": 0},\n",
    "            x=0.1,\n",
    "            y=0,\n",
    "            len=0.9,\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "#fig.write_html(\"animacion_lidar_3d.html\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
